{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my imports, to make sure it works fine on Jupyter Notebook\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.context import SQLContext\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.functions import lit\n",
    "# from pyspark.sql.window import Window\n",
    "# import pyspark.sql.functions as F\n",
    "# from datetime import datetime\n",
    "# import pyspark.sql.types as T \n",
    "# from pyspark.sql.functions import split, explode\n",
    "\n",
    "#To get spark. working without throwing a NameError\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark as ps # Call this only after findspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#ACTUAL FUNCTION\n",
    "\n",
    "#for renaming the columns\n",
    "from functools import reduce\n",
    "\n",
    "#allow us to use SQL Count function for aggregation in groupBY\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "#allow us to read csv to dataframe\n",
    "import pandas as pd #need Pandas\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### common pyspark import statements\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T \n",
    "from pyspark.sql.functions import split, explode\n",
    "### other essnetial import statements\n",
    "import argparse\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#my imports\n",
    "from pyspark.sql.types import IntegerType\n",
    "def main(inputFile, outputFile, configFile, contentMapping):\n",
    "    #df = spark.read.parquet(inputFile+'/*').dropDuplicates().na.drop()\n",
    "    df = inputFile #is just a df\n",
    "    #contentMapping = spark.read.parquet(contentMapping+'/*') #right formatting?\n",
    "    \n",
    "    #get rid of \".mp3\" in item_name\n",
    "    df = df.withColumnRenamed(\"item_name\", \"to_del\")\n",
    "    df = df.withColumn(\"item_name\", F.split(df['to_del'], '\\.')[0])\n",
    "    df = df.drop('to_del')\n",
    "    \n",
    "    #turn Content Mapping String Length to TimeDelta Object\n",
    "    strp_time = udf (lambda x: datetime.strptime(x, \"%M:%S\"))\n",
    "    time_delta = udf (lambda y: timedelta(minutes = y.minute, seconds = y.second))\n",
    "    \n",
    "    contentMapping = contentMapping.withColumn(\"strptime\", strp_time(F.col(\"Length\")))\n",
    "    contentMapping = contentMapping.withColumn(\"Content Length\", time_delta(F.col(\"strptime\")))\n",
    "    contentMapping = contentMapping.drop('strpTime')\n",
    "    contentMapping = contentMapping.withColumnRenamed(\"Title\", \"item_name\")\n",
    "    \n",
    "    #Merge df and contentMapping\n",
    "    df = df.join(contentMapping, [\"item_name\"], \"outer\")\n",
    "    \n",
    "    #get time played for\n",
    "    df = df.withColumn(\"Played For\", F.unix_timestamp(df[\"end\"]) - F.unix_timestamp(df[\"start\"]))\n",
    "    \n",
    "    #get total seconds of song as String, convert to bigInt\n",
    "    df = df.withColumn(\"Song Duration Str\", F.regexp_extract(df[\"Content Length\"], \"(?<=total: )(.*)(?= seconds)\", 0))\n",
    "    df = df.withColumn(\"Song Duration Int\", df[\"Song Duration Str\"].cast(IntegerType()))\n",
    "    \n",
    "\n",
    "    #Let's get Percentage Played\n",
    "    df = df.withColumn(\"PercentPlayed\", df[\"Played For\"] / df[\"Song Duration Int\"])\n",
    "    \n",
    "    #Let's keep only the columns we need at this point\n",
    "    df = df.select([\"device_id\", \"item_name\", \"PercentPlayed\"])\n",
    "\n",
    "    #assign weights based on Percent Played\n",
    "    df = df.withColumn(\n",
    "    'weight',\n",
    "    F.when((F.col(\"PercentPlayed\") >= 0.0) & (F.col(\"PercentPlayed\") < 0.25), 0.0)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.25) & (F.col(\"PercentPlayed\") < 0.50), 0.33)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.50) & (F.col(\"PercentPlayed\") < 0.75), 0.66)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.75) & (F.col(\"PercentPlayed\") <= 1.00), 0.66)\\\n",
    "    .otherwise(-999.999)\n",
    "    )\n",
    "    #drop the rows with invalid percent played\n",
    "    df = df.filter((df.weight != -999.999))\n",
    "\n",
    "    #df.write.parquet(outputFile) # Write onto output Parquet\n",
    "    return df #return changed df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "df_pd = spark.createDataFrame(\n",
    "    [#shouldn't matter that I use int for user id's as they are never touched in this function \n",
    "        (0, 'Black Beauty Part 1 The Spring Hunt.mp3', \n",
    "         datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p'), \n",
    "         datetime.strptime('Jun 1 2005  1:30PM', '%b %d %Y %I:%M%p'),\n",
    "        False), # create your data here, be consistent in the types.\n",
    "        (1, \"The Peacock.mp3\", \n",
    "         datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p'), \n",
    "         datetime.strptime('Jun 1 2005  1:50PM', '%b %d %Y %I:%M%p'),\n",
    "        True),\n",
    "        (1, \"Phonics.mp3\", \n",
    "         datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p'), \n",
    "         datetime.strptime('Jun 1 2005  1:34PM', '%b %d %Y %I:%M%p'),\n",
    "        False), # create your data here, be consistent in the types.\n",
    "        (1, \"Pizza Moon.mp3\", \n",
    "         datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p'), \n",
    "         datetime.strptime('Jun 1 2005  1:34PM', '%b %d %Y %I:%M%p'),\n",
    "        True),\n",
    "        (1, \"Pop Goes the Weasel2.mp3\", \n",
    "         datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p'), \n",
    "         datetime.strptime('Jun 1 2005  1:34PM', '%b %d %Y %I:%M%p'),\n",
    "        False), # create your data here, be consistent in the types.\n",
    "    ],\n",
    "    ['device_id', 'item_name', \"start\", \"end\", \"reach_end_of_stream\"] # add your columns label here\n",
    ")\n",
    "content_mapping = spark.createDataFrame([\n",
    "    (\"Black Beauty Part 1 The Spring Hunt\", \"4:15\"),\n",
    "    (\"The Peacock\", \"2:35\"),\n",
    "    (\"Phonics\", \"4:01\"),\n",
    "    (\"Pizza Moon\", \"3:20\"),\n",
    "    (\"Pop Goes the Weasel2\", \"0:41\"),],\n",
    "    [\"Title\", \"Length\"]\n",
    "    )\n",
    "#         # call class function to be tested\n",
    "output_spark = main(df_pd, \"dd\", \"dd\", content_mapping)\n",
    "        # convert output spark DF to pandas DF\n",
    "        output_pd = output_spark.toPandas() #taking in a parquet, so switch that parquet to pandas dataframe\n",
    "        # expected output\n",
    "        expected = pd.DataFrame({'device_id':['1','1'],\n",
    "            'item_name':['Phonics','Pizza Moon'],\n",
    "            'PercentPlayed':[0.24896265560165975, 0.3],\n",
    "            'weight':[0.0,0.33]})\n",
    "        # assert output == expected output\n",
    "        self.assert_frame_equal_with_sort(output_pd,expected,['device_id','item_name', 'PercentPlayed', 'weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+-------------------+-------------------+\n",
      "|device_id|           item_name|              start|                end|reach_end_of_stream|\n",
      "+---------+--------------------+-------------------+-------------------+-------------------+\n",
      "|        0|Black Beauty Part...|2005-06-01 13:33:00|2005-06-01 13:30:00|              false|\n",
      "|        1|     The Peacock.mp3|2005-06-01 13:33:00|2005-06-01 13:50:00|               true|\n",
      "|        1|         Phonics.mp3|2005-06-01 13:33:00|2005-06-01 13:34:00|              false|\n",
      "|        1|      Pizza Moon.mp3|2005-06-01 13:33:00|2005-06-01 13:34:00|               true|\n",
      "|        1|Pop Goes the Weas...|2005-06-01 13:33:00|2005-06-01 13:34:00|              false|\n",
      "+---------+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|               Title|Length|\n",
      "+--------------------+------+\n",
      "|Black Beauty Part...|  4:15|\n",
      "|         The Peacock|  2:35|\n",
      "|             Phonics|  4:01|\n",
      "|          Pizza Moon|  3:20|\n",
      "|Pop Goes the Weasel2|  0:41|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_mapping.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------------+------+\n",
      "|device_id| item_name|      PercentPlayed|weight|\n",
      "+---------+----------+-------------------+------+\n",
      "|        1|   Phonics|0.24896265560165975|   0.0|\n",
      "|        1|Pizza Moon|                0.3|  0.33|\n",
      "+---------+----------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
