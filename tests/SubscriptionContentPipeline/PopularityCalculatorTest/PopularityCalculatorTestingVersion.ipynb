{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in/outputs dataframes instead of Parquets to make testing more convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my imports, to make sure it works fine on Jupyter Notebook\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.context import SQLContext\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.functions import lit\n",
    "# from pyspark.sql.window import Window\n",
    "# import pyspark.sql.functions as F\n",
    "# from datetime import datetime\n",
    "# import pyspark.sql.types as T \n",
    "# from pyspark.sql.functions import split, explode\n",
    "\n",
    "#To get spark. working without throwing a NameError\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark as ps # Call this only after findspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#ACTUAL FUNCTION\n",
    "\n",
    "#for renaming the columns\n",
    "from functools import reduce\n",
    "\n",
    "#allow us to use SQL Count function for aggregation in groupBY\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "#allow us to read csv to dataframe\n",
    "import pandas as pd #need Pandas\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### common pyspark import statements\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T \n",
    "from pyspark.sql.functions import split, explode\n",
    "### other essnetial import statements\n",
    "import argparse\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#my imports\n",
    "from pyspark.sql.types import IntegerType\n",
    "def main(inputFile, outputFile, configFile, contentMapping):\n",
    "    #df = spark.read.parquet(inputFile+'/*').dropDuplicates().na.drop()\n",
    "    df = inputFile #is just a df\n",
    "    #contentMapping = spark.read.parquet(contentMapping+'/*') #right formatting?\n",
    "    \n",
    "    #get rid of \".mp3\" in item_name\n",
    "    df = df.withColumnRenamed(\"item_name\", \"to_del\")\n",
    "    df = df.withColumn(\"item_name\", F.split(df['to_del'], '\\.')[0])\n",
    "    df = df.drop('to_del')\n",
    "    \n",
    "    #turn Content Mapping String Length to TimeDelta Object\n",
    "    strp_time = udf (lambda x: datetime.strptime(x, \"%M:%S\"))\n",
    "    time_delta = udf (lambda y: timedelta(minutes = y.minute, seconds = y.second))\n",
    "    \n",
    "    contentMapping = contentMapping.withColumn(\"strptime\", strp_time(F.col(\"Length\")))\n",
    "    contentMapping = contentMapping.withColumn(\"Content Length\", time_delta(F.col(\"strptime\")))\n",
    "    contentMapping = contentMapping.drop('strpTime')\n",
    "    contentMapping = contentMapping.withColumnRenamed(\"Title\", \"item_name\")\n",
    "    \n",
    "    #Merge df and contentMapping\n",
    "    df = df.join(contentMapping, [\"item_name\"], \"outer\")\n",
    "    \n",
    "    #get time played for\n",
    "    df = df.withColumn(\"Played For\", F.unix_timestamp(df[\"end\"]) - F.unix_timestamp(df[\"start\"]))\n",
    "    \n",
    "    #get total seconds of song as String, convert to bigInt\n",
    "    df = df.withColumn(\"Song Duration Str\", F.regexp_extract(df[\"Content Length\"], \"(?<=total: )(.*)(?= seconds)\", 0))\n",
    "    df = df.withColumn(\"Song Duration Int\", df[\"Song Duration Str\"].cast(IntegerType()))\n",
    "    \n",
    "\n",
    "    #Let's get Percentage Played\n",
    "    df = df.withColumn(\"PercentPlayed\", df[\"Played For\"] / df[\"Song Duration Int\"])\n",
    "    \n",
    "    #Let's keep only the columns we need at this point\n",
    "    df = df.select([\"device_id\", \"item_name\", \"PercentPlayed\"])\n",
    "\n",
    "    #assign weights based on Percent Played\n",
    "    df = df.withColumn(\n",
    "    'weight',\n",
    "    F.when((F.col(\"PercentPlayed\") >= 0.0) & (F.col(\"PercentPlayed\") < 0.25), 0.0)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.25) & (F.col(\"PercentPlayed\") < 0.50), 0.33)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.50) & (F.col(\"PercentPlayed\") < 0.75), 0.66)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.75) & (F.col(\"PercentPlayed\") <= 1.00), 0.66)\\\n",
    "    .otherwise(-999.999)\n",
    "    )\n",
    "    #drop the rows with invalid percent played\n",
    "    df = df.filter((df.weight != -999.999))\n",
    "\n",
    "    #df.write.parquet(outputFile) # Write onto output Parquet\n",
    "    return df #return changed df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
