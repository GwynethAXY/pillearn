{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### common pyspark import statements\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T \n",
    "from pyspark.sql.functions import split, explode\n",
    "### other essnetial import statements\n",
    "import argparse\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#my imports\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "'''\n",
    "\tStandardize code to the following structure: everything to be in functions except pyspark envrionment setup\n",
    "\t\t- environment setup\n",
    "\t\t- user defined functions\n",
    "\t\t- main function\n",
    "\t\t- program start point\n",
    "''' \n",
    "\n",
    "# setup\n",
    "conf = SparkConf()\n",
    "conf.setAppName('pillar')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel('WARN')\n",
    "sql_context = SQLContext(sc)\n",
    "spark = SparkSession.builder.appName('pillar').getOrCreate()\n",
    "\n",
    "# main function: should always take in input file, output file and config file\n",
    "def main(inputFile, outputFile, configFile):\n",
    "\n",
    "    df = spark.read.parquet(inputFile+'/*').dropDuplicates().na.drop()\n",
    "    \n",
    "     #DF for Item Play Started\n",
    "    start = df.filter(col('event').isin(['ITEM_PLAY_STARTED']))\n",
    "    #split json string and get just the name\n",
    "    start = start.withColumn('Content Name', split(df['payload'], ':')[2])\n",
    "    name_UDF = udf(lambda x:x[1:-2],StringType()) \n",
    "    start = start.withColumn('Content Name', name_UDF('Content Name'))\n",
    "    #turn Time Start String to easy to use Time Stamp Object\n",
    "    start = start.withColumn('Time Start', df['time'].cast(\"timestamp\"))\n",
    "    #Select just the columns we need\n",
    "    cols = [\"device_id\", \"Content Name\", \"Time Start\"]\n",
    "    start = start.select(*cols)\n",
    "    \n",
    "    \n",
    "    #DF for Item Play Finished\n",
    "    finished = df.filter(col('event').isin(['ITEM_PLAY_FINISHED']))\n",
    "    #split json string and get just the name\n",
    "    finished = finished.withColumn('Content Name', split(df['payload'], ':')[3])\n",
    "    finished = finished.withColumn('Content Name', name_UDF('Content Name'))\n",
    "    \n",
    "    #split json string and get whether content was played in entirety\n",
    "    finished = finished.withColumn('reach_end_of_stream', split(df['payload'], ':')[1])\n",
    "    stream_end_UDF = udf(lambda x:x[0:5],StringType()) \n",
    "    finished = finished.withColumn('reach_end_of_stream', stream_end_UDF('reach_end_of_stream'))\n",
    "    \n",
    "    #Get rid of white space\n",
    "    finished = finished.withColumn(\"reach_end_of_stream\", trim(col(\"reach_end_of_stream\")))\n",
    "    \n",
    "    #Convert True/False strings to actual boolean values\n",
    "    finished = finished.withColumn(\n",
    "    'reach_end_of_stream',\n",
    "    F.when(F.col(\"reach_end_of_stream\") == \"true\", True)\\\n",
    "    .otherwise(False)\n",
    "    )\n",
    "    #turn Time End String to easy to use Time Stamp Object\n",
    "    finished = finished.withColumn('Time End', df['time'].cast(\"timestamp\"))\n",
    "    #Select just the columns we need\n",
    "    cols = [\"device_id\", \"Content Name\", \"Time End\", \"reach_end_of_stream\"]\n",
    "    finished = finished.select(*cols)\n",
    "    \n",
    "    #combine two dataframes for our transformed Schema\n",
    "    transformed = start.join(finished, on=[\"device_id\", \"Content Name\"], how='left_outer')\n",
    "    \n",
    "    #Make sure Time Start before time end\n",
    "    transformed = transformed.where(col(\"Time Start\") <= col(\"Time End\"))\n",
    "    \n",
    "    #Convert time stamps to unix\n",
    "    #transformed = transformed.withColumn('Time Start', F.unix_timestamp('Time Start'))\n",
    "    #transformed = transformed.withColumn('Time End', F.unix_timestamp('Time End'))\n",
    "    \n",
    "    #Get correct Time Ends\n",
    "    def getEndTime(end_time_list):\n",
    "        return end_time_list[0]\n",
    "    \n",
    "    end_time_udf = udf(lambda end_time_list: end_time_list[0], TimestampType())\n",
    "    df = transformed.withColumn(\"end_time_list\", F.collect_list(\"Time End\").over(Window.partitionBy(\"device_id\",'Content Name','Time Start', \"reach_end_of_stream\").orderBy('Time End')))\n",
    "    df = df.groupBy('device_id','Time Start','Content Name', \"reach_end_of_stream\").agg(F.max('end_time_list').alias('end_time_list'))\n",
    "    #Still gets laggy here running the udf that takes first item of list (aka the smallest date time)\n",
    "    df = df.withColumn('Time End', end_time_udf(\"end_time_list\"))\n",
    "    df = df.drop('end_time_list')\n",
    "    \n",
    "    #rename columns + reorder\n",
    "    df = df.withColumnRenamed(\"Time Start\", \"start\").withColumnRenamed(\"Time End\", \"end\").withColumnRenamed(\"Content Name\", \"item_name\")\n",
    "    df = df.select(\"device_id\", \"item_name\", \"start\", \"end\", \"reach_end_of_stream\")\n",
    "\n",
    "    df.write.parquet(outputFile) # Write onto output Parquet\n",
    "\n",
    "\n",
    "# program start point\n",
    "if __name__ == \"__main__\":\n",
    "    ''' \n",
    "        Add arguments to script during execution on command line\n",
    "        Example of how to run the script: spark-submit template.py -i input.parquet -o output.parquet -c config.ini\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser() \n",
    "    parser.add_argument('-i', '--input', required=True)\n",
    "    parser.add_argument('-o', '--output', required=True)\n",
    "    parser.add_argument('-c', '--config', required=True)\n",
    "    args = parser.parse_args()\n",
    "    config = ConfigParser()\n",
    "    config.read(args.config)\n",
    "    main(args.input,args.output,config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
