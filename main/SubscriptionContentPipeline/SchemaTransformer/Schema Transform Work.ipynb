{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4/13/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my imports, to make sure it works fine on Jupyter Notebook\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark as ps # Call this only after findspark\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "import pyspark.sql.types as T \n",
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "#To get spark. working without throwing a NameError\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark as ps # Call this only after findspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#ACTUAL FUNCTION\n",
    "\n",
    "#for renaming the columns\n",
    "from functools import reduce\n",
    "\n",
    "#allow us to use SQL Count function for aggregation in groupBY\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "#allow us to read csv to dataframe\n",
    "import pandas as pd #need Pandas\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "t = datetime.strptime(\"2:00\", \"%M:%S\")\n",
    "delta = timedelta(minutes=t.minute, seconds = t.second)\n",
    "\n",
    "inp = spark.createDataFrame(\n",
    "    [\n",
    "        (123, \"ITEM_PLAY_STARTED\", '{\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"}',\n",
    "        \"2020-02-07T18:22:29.877Z\"), # create your data here, be consistent in the types.\n",
    "        (123, \"ITEM_PLAY_FINISHED\", '{\"did_reach_end_of_stream\":false,\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"}',\n",
    "        \"2020-02-07T18:22:48.695Z\"),\n",
    "        (123, \"ITEM_PLAY_STARTED\", '{\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"}',\n",
    "        \"2020-02-07T18:23:29.877Z\"), # create your data here, be consistent in the types.\n",
    "        (123, \"ITEM_PLAY_FINISHED\", '{\"did_reach_end_of_stream\":false,\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"}',\n",
    "        \"2020-02-07T18:23:48.695Z\"),\n",
    "        (123, \"ITEM_PLAY_STARTED\", '{\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"}',\n",
    "        \"2020-02-07T18:24:29.877Z\"), # create your data here, be consistent in the types.\n",
    "        (123, \"ITEM_PLAY_FINISHED\", '{\"did_reach_end_of_stream\":true,\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"}',\n",
    "        \"2020-02-07T18:24:48.695Z\")\n",
    "    ],\n",
    "    ['device_id', 'event', \"payload\", \"time\"] # add your columns label here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp.write.parquet(\"sampleInput.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-------------------------------------------------------------------------------------------+------------------------+\n",
      "|device_id|event             |payload                                                                                    |time                    |\n",
      "+---------+------------------+-------------------------------------------------------------------------------------------+------------------------+\n",
      "|123      |ITEM_PLAY_STARTED |{\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"}                                |2020-02-07T18:22:29.877Z|\n",
      "|123      |ITEM_PLAY_FINISHED|{\"did_reach_end_of_stream\":false,\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"}|2020-02-07T18:22:48.695Z|\n",
      "|123      |ITEM_PLAY_STARTED |{\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"}                                |2020-02-07T18:23:29.877Z|\n",
      "|123      |ITEM_PLAY_FINISHED|{\"did_reach_end_of_stream\":false,\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"}|2020-02-07T18:23:48.695Z|\n",
      "|123      |ITEM_PLAY_STARTED |{\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"}                                |2020-02-07T18:24:29.877Z|\n",
      "|123      |ITEM_PLAY_FINISHED|{\"did_reach_end_of_stream\":true,\"category\":\"Songs\",\"item_name\":\"Pop Goes the Weasel2.mp3\"} |2020-02-07T18:24:48.695Z|\n",
      "+---------+------------------+-------------------------------------------------------------------------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(inputFile, outputFile, configFile):\n",
    "    from pyspark.sql.functions import col\n",
    "    from pyspark.sql.functions import split\n",
    "    from pyspark.sql.functions import substring\n",
    "    from pyspark.sql.types import StringType\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.functions import trim\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "#     df = spark.read.parquet(inputFile+'/*').dropDuplicates().na.drop()\n",
    "#     contentMapping = spark.read.parquet(contentMapping+'/*') #right formatting?\n",
    "    df = inputFile\n",
    "    \n",
    "    #DF for Item Play Started\n",
    "    start = df.filter(col('event').isin(['ITEM_PLAY_STARTED']))\n",
    "    #split json string and get just the name\n",
    "    start = start.withColumn('Content Name', split(df['payload'], ':')[2])\n",
    "    name_UDF = udf(lambda x:x[1:-2],StringType()) \n",
    "    start = start.withColumn('Content Name', name_UDF('Content Name'))\n",
    "    #turn Time Start String to easy to use Time Stamp Object\n",
    "    start = start.withColumn('Time Start', df['time'].cast(\"timestamp\"))\n",
    "    #Select just the columns we need\n",
    "    cols = [\"device_id\", \"Content Name\", \"Time Start\"]\n",
    "    start = start.select(*cols)\n",
    "    \n",
    "    \n",
    "    #DF for Item Play Finished\n",
    "    finished = df.filter(col('event').isin(['ITEM_PLAY_FINISHED']))\n",
    "    #split json string and get just the name\n",
    "    finished = finished.withColumn('Content Name', split(df['payload'], ':')[3])\n",
    "    finished = finished.withColumn('Content Name', name_UDF('Content Name'))\n",
    "    \n",
    "    #split json string and get whether content was played in entirety\n",
    "    finished = finished.withColumn('reach_end_of_stream', split(df['payload'], ':')[1])\n",
    "    stream_end_UDF = udf(lambda x:x[0:5],StringType()) \n",
    "    finished = finished.withColumn('reach_end_of_stream', stream_end_UDF('reach_end_of_stream'))\n",
    "    \n",
    "    #Get rid of white space\n",
    "    finished = finished.withColumn(\"reach_end_of_stream\", trim(col(\"reach_end_of_stream\")))\n",
    "    \n",
    "    #Convert True/False strings to actual boolean values\n",
    "    finished = finished.withColumn(\n",
    "    'reach_end_of_stream',\n",
    "    F.when(F.col(\"reach_end_of_stream\") == \"true\", True)\\\n",
    "    .otherwise(False)\n",
    "    )\n",
    "    #turn Time End String to easy to use Time Stamp Object\n",
    "    finished = finished.withColumn('Time End', df['time'].cast(\"timestamp\"))\n",
    "    #Select just the columns we need\n",
    "    cols = [\"device_id\", \"Content Name\", \"Time End\", \"reach_end_of_stream\"]\n",
    "    finished = finished.select(*cols)\n",
    "    \n",
    "    #combine two dataframes for our transformed Schema\n",
    "    transformed = start.join(finished, on=[\"device_id\", \"Content Name\"], how='left_outer')\n",
    "    \n",
    "    #Make sure Time Start before time end\n",
    "    transformed = transformed.where(col(\"Time Start\") <= col(\"Time End\"))\n",
    "    \n",
    "    #Convert time stamps to unix\n",
    "    #transformed = transformed.withColumn('Time Start', F.unix_timestamp('Time Start'))\n",
    "    #transformed = transformed.withColumn('Time End', F.unix_timestamp('Time End'))\n",
    "    \n",
    "    #Get correct Time Ends\n",
    "    def getEndTime(end_time_list):\n",
    "        return end_time_list[0]\n",
    "    \n",
    "    end_time_udf = udf(lambda end_time_list: end_time_list[0], TimestampType())\n",
    "    df = transformed.withColumn(\"end_time_list\", F.collect_list(\"Time End\").over(Window.partitionBy(\"device_id\",'Content Name','Time Start', \"reach_end_of_stream\").orderBy('Time End')))\n",
    "    df = df.groupBy('device_id','Time Start','Content Name', \"reach_end_of_stream\").agg(F.max('end_time_list').alias('end_time_list'))\n",
    "    #Still gets laggy here running the udf that takes first item of list (aka the smallest date time)\n",
    "    df = df.withColumn('Time End', end_time_udf(\"end_time_list\"))\n",
    "    df = df.drop('end_time_list')\n",
    "    \n",
    "    #rename columns + reorder\n",
    "    df = df.withColumnRenamed(\"Time Start\", \"start\").withColumnRenamed(\"Time End\", \"end\").withColumnRenamed(\"Content Name\", \"item_name\")\n",
    "    df = df.select(\"device_id\", \"item_name\", \"start\", \"end\", \"reach_end_of_stream\")\n",
    "    #df = df.join(ref,[df.device_id == ref.device_id2]).drop('device_id2')\n",
    "\n",
    "    \n",
    "    #Find min within Time End list\n",
    "#     temp = transformed.groupby(\"Time Start\").agg(F.collect_list(\"Time End\").alias(\"Time End List\"))\n",
    "#     temp = temp.agg(F.min(\"Time End List\").alias(\"Time End List\"))\n",
    "#     min_time_UDF = udf(lambda x: __builtins__.min(x), TimestampType()) \n",
    "#     #***************************************************************************************\n",
    "#     #***This is where it gets incredibly laggy. For some reason min. UDF doesn't work well. \n",
    "#     #***************************************************************************************\n",
    "#     temp = temp.withColumn('Time End', min_time_UDF('Time End List'))\n",
    "#     cols = [\"Time Start\", \"Time End\"]\n",
    "#     temp = temp.select(*cols)\n",
    "\n",
    "\n",
    "    \n",
    "    #Combine temp and transformed \n",
    "    #transformed = transformed.join(temp, on=[\"Time Start\", \"Time End\"], how='right')\n",
    "    #transformed = transformed.select(\"device_id\",\"Content Name\",\"Time Start\",\"city\")\n",
    "\n",
    "    \n",
    "#     event_loggerino = pd.merge(play_started, play_finished, on = [\"device_id\", \"Content Name\"]) #merge two df's together on id/content name\n",
    "#     event_loggerino = event_loggerino[ [\"device_id\", \"Content Name\", \"Time Start\", \"Time End\", \"Content Consumed Entirely?\"] ] #columns wanted in the order I want them\n",
    "#     event_loggerino[\"Content Consumed Entirely?\"] = event_loggerino[\"Content Consumed Entirely?\"].astype(int) #Convert True/False values to 1/0\n",
    "\n",
    "#     df.write.parquet(outputFile) # Write onto output Parquet\n",
    "    return df#.show(truncate = False) #SEE NOTE WRITTEN IN ALL CAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = main(inp, \"dd\", \"dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.write.parquet(\"sampleOutput.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
