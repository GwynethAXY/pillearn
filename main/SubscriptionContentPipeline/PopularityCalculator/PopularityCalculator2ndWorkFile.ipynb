{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my imports, to make sure it works fine on Jupyter Notebook\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.context import SQLContext\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.functions import lit\n",
    "# from pyspark.sql.window import Window\n",
    "# import pyspark.sql.functions as F\n",
    "# from datetime import datetime\n",
    "# import pyspark.sql.types as T \n",
    "# from pyspark.sql.functions import split, explode\n",
    "\n",
    "#To get spark. working without throwing a NameError\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark as ps # Call this only after findspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#ACTUAL FUNCTION\n",
    "\n",
    "#for renaming the columns\n",
    "from functools import reduce\n",
    "\n",
    "#allow us to use SQL Count function for aggregation in groupBY\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "#allow us to read csv to dataframe\n",
    "import pandas as pd #need Pandas\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pillar, master=local[*]) created by __init__ at <ipython-input-3-01abe7828252>:31 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ad692b8c80b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pillar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetLogLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'WARN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0msql_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mD:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    314\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 316\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pillar, master=local[*]) created by __init__ at <ipython-input-3-01abe7828252>:31 "
     ]
    }
   ],
   "source": [
    "### common pyspark import statements\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T \n",
    "from pyspark.sql.functions import split, explode\n",
    "### other essnetial import statements\n",
    "import argparse\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#my imports\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "'''\n",
    "\tStandardize code to the following structure: everything to be in functions except pyspark envrionment setup\n",
    "\t\t- environment setup\n",
    "\t\t- user defined functions\n",
    "\t\t- main function\n",
    "\t\t- program start point\n",
    "''' \n",
    "\n",
    "# setup\n",
    "conf = SparkConf()\n",
    "conf.setAppName('pillar')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel('WARN')\n",
    "sql_context = SQLContext(sc)\n",
    "spark = SparkSession.builder.appName('pillar').getOrCreate()\n",
    "\n",
    "\n",
    "# main function: should always take in input file, output file and config file\n",
    "def main(inputFile, outputFile, configFile, contentMapping):\n",
    "    df = spark.read.parquet(inputFile+'/*').dropDuplicates().na.drop()\n",
    "    contentMapping = spark.read.parquet(contentMapping+'/*') #right formatting?\n",
    "    \n",
    "    #get rid of \".mp3\" in item_name\n",
    "    df = df.withColumnRenamed(\"item_name\", \"to_del\")\n",
    "    df = df.withColumn(\"item_name\", F.split(df['to_del'], '\\.')[0])\n",
    "    df = df.drop('to_del')\n",
    "    \n",
    "    #turn Content Mapping String Length to TimeDelta Object\n",
    "    strp_time = udf (lambda x: datetime.strptime(x, \"%M:%S\"))\n",
    "    time_delta = udf (lambda y: timedelta(minutes = y.minute, seconds = y.second))\n",
    "    \n",
    "    contentMapping = contentMapping.withColumn(\"strptime\", strp_time(F.col(\"Length\")))\n",
    "    contentMapping = contentMapping.withColumn(\"Content Length\", time_delta(F.col(\"strptime\")))\n",
    "    contentMapping = contentMapping.drop('strpTime')\n",
    "    contentMapping = contentMapping.withColumnRenamed(\"Title\", \"item_name\")\n",
    "    \n",
    "    #Merge df and contentMapping\n",
    "    df = df.join(contentMapping, [\"item_name\"], \"outer\")\n",
    "    \n",
    "    #get time played for\n",
    "    df = df.withColumn(\"Played For\", F.unix_timestamp(df[\"end\"]) - F.unix_timestamp(df[\"start\"]))\n",
    "    \n",
    "    #get total seconds of song as String, convert to bigInt\n",
    "    df = df.withColumn(\"Song Duration Str\", F.regexp_extract(df[\"Content Length\"], \"(?<=total: )(.*)(?= seconds)\", 0))\n",
    "    df = df.withColumn(\"Song Duration Int\", df[\"Song Duration Str\"].cast(IntegerType()))\n",
    "    \n",
    "\n",
    "    #Let's get Percentage Played\n",
    "    df = df.withColumn(\"PercentPlayed\", df[\"Played For\"] / df[\"Song Duration Int\"])\n",
    "    \n",
    "    #Let's keep only the columns we need at this point\n",
    "    df = df.select([\"device_id\", \"item_name\", \"PercentPlayed\"])\n",
    "\n",
    "    #assign weights based on Percent Played\n",
    "    df = df.withColumn(\n",
    "    'weight',\n",
    "    F.when((F.col(\"PercentPlayed\") >= 0.0) & (F.col(\"PercentPlayed\") < 0.25), 0.0)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.25) & (F.col(\"PercentPlayed\") < 0.50), 0.33)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.50) & (F.col(\"PercentPlayed\") < 0.75), 0.66)\\\n",
    "    .otherwise(1.0)\n",
    "    )\n",
    "\n",
    "    df.write.parquet(outputFile) # Write onto output Parquet\n",
    "\n",
    "\n",
    "# program start point\n",
    "if __name__ == \"__main__\":\n",
    "    ''' \n",
    "        Add arguments to script during execution on command line\n",
    "        Example of how to run the script: spark-submit template.py -i input.parquet -o output.parquet -c config.ini\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser() \n",
    "    parser.add_argument('-i', '--input', required=True)\n",
    "    parser.add_argument('-o', '--output', required=True)\n",
    "    parser.add_argument('-c', '--config', required=True)\n",
    "    parser.add_argument('-cm', '--contentmapping', required=True) #added on, so contentMapping is required\n",
    "    args = parser.parse_args()\n",
    "    config = ConfigParser()\n",
    "    config.read(args.config)\n",
    "    main(args.input,args.output,config, args.contentmapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(inputFile, outputFile, configFile, contentMapping):\n",
    "    df = spark.read.parquet(inputFile+'/*').dropDuplicates().na.drop()\n",
    "    contentMapping = spark.read.parquet(contentMapping+'/*') #right formatting?\n",
    "    \n",
    "    #get rid of \".mp3\" in item_name\n",
    "    df = df.withColumnRenamed(\"item_name\", \"to_del\")\n",
    "    df = df.withColumn(\"item_name\", F.split(df['to_del'], '\\.')[0])\n",
    "    df = df.drop('to_del')\n",
    "    \n",
    "    #turn Content Mapping String Length to TimeDelta Object\n",
    "    strp_time = udf (lambda x: datetime.strptime(x, \"%M:%S\"))\n",
    "    time_delta = udf (lambda y: timedelta(minutes = y.minute, seconds = y.second))\n",
    "    \n",
    "    contentMapping = contentMapping.withColumn(\"strptime\", strp_time(F.col(\"Length\")))\n",
    "    contentMapping = contentMapping.withColumn(\"Content Length\", time_delta(F.col(\"strptime\")))\n",
    "    contentMapping = contentMapping.drop('strpTime')\n",
    "    contentMapping = contentMapping.withColumnRenamed(\"Title\", \"item_name\")\n",
    "    \n",
    "    #Merge df and contentMapping\n",
    "    df = df.join(contentMapping, [\"item_name\"], \"outer\")\n",
    "    \n",
    "    #get time played for\n",
    "    df = df.withColumn(\"Played For\", F.unix_timestamp(df[\"end\"]) - F.unix_timestamp(df[\"start\"]))\n",
    "    \n",
    "    #get total seconds of song as String, convert to bigInt\n",
    "    df = df.withColumn(\"Song Duration Str\", F.regexp_extract(df[\"Content Length\"], \"(?<=total: )(.*)(?= seconds)\", 0))\n",
    "    df = df.withColumn(\"Song Duration Int\", df[\"Song Duration Str\"].cast(IntegerType()))\n",
    "    \n",
    "\n",
    "    #Let's get Percentage Played\n",
    "    df = df.withColumn(\"PercentPlayed\", df[\"Played For\"] / df[\"Song Duration Int\"])\n",
    "    \n",
    "    #Let's keep only the columns we need at this point\n",
    "    df = df.select([\"device_id\", \"item_name\", \"PercentPlayed\"])\n",
    "\n",
    "    #assign weights based on Percent Played\n",
    "    df = df.withColumn(\n",
    "    'weight',\n",
    "    F.when((F.col(\"PercentPlayed\") >= 0.0) & (F.col(\"PercentPlayed\") < 0.25), 0.0)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.25) & (F.col(\"PercentPlayed\") < 0.50), 0.33)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.50) & (F.col(\"PercentPlayed\") < 0.75), 0.66)\\\n",
    "    .otherwise(1.0)\n",
    "    )\n",
    "\n",
    "    df.write.parquet(outputFile) # Write onto output Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"input.parquet\", \"sampleOutput2.parquet\", \"configFile Placeholder, never use it\", \"contentMapping.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputCheck = spark.read.parquet(\"sampleOutput2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------+------+\n",
      "|device_id|           item_name|PercentPlayed|weight|\n",
      "+---------+--------------------+-------------+------+\n",
      "|        1|Pop Goes the Weasel2|          2.0|   1.0|\n",
      "|        1|Pop Goes the Weasel2|          0.5|  0.66|\n",
      "+---------+--------------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputCheck.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
