{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### common pyspark import statements\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T \n",
    "from pyspark.sql.functions import split, explode\n",
    "### other essnetial import statements\n",
    "import argparse\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#my imports\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "'''\n",
    "\tStandardize code to the following structure: everything to be in functions except pyspark envrionment setup\n",
    "\t\t- environment setup\n",
    "\t\t- user defined functions\n",
    "\t\t- main function\n",
    "\t\t- program start point\n",
    "''' \n",
    "\n",
    "# setup\n",
    "conf = SparkConf()\n",
    "conf.setAppName('pillar')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel('WARN')\n",
    "sql_context = SQLContext(sc)\n",
    "spark = SparkSession.builder.appName('pillar').getOrCreate()\n",
    "\n",
    "#config accessor\n",
    "class popularityCalculator(object):\n",
    "    def __init__(self, config):\n",
    "        self.first = float(config['weights']['FIRST'])\n",
    "        self.second = float(config['weights']['SECOND'])\n",
    "        self.third = float(config['weights']['THIRD'])\n",
    "        self.fourth = float(config['weights']['FOURTH'])\n",
    "\n",
    "# main function: should always take in input file, output file and config file\n",
    "def main(inputFile, outputFile, configFile, contentMapping):\n",
    "    \n",
    "    #config\n",
    "    uc = popularityCalculator(configFile)\n",
    "    \n",
    "    df = spark.read.parquet(inputFile+'/*').dropDuplicates().na.drop()\n",
    "    contentMapping = spark.read.parquet(contentMapping+'/*') #right formatting?\n",
    "    \n",
    "     #get rid of \".mp3\" in item_name\n",
    "    df = df.withColumnRenamed(\"item_name\", \"to_del\")\n",
    "    df = df.withColumn(\"item_name\", F.split(df['to_del'], '\\.')[0])\n",
    "    df = df.drop('to_del')\n",
    "    \n",
    "    #turn Content Mapping String Length to TimeDelta Object\n",
    "    strp_time = udf (lambda x: datetime.strptime(x, \"%M:%S\"))\n",
    "    time_delta = udf (lambda y: timedelta(minutes = y.minute, seconds = y.second))\n",
    "    \n",
    "    contentMapping = contentMapping.withColumn(\"strptime\", strp_time(F.col(\"Length\")))\n",
    "    contentMapping = contentMapping.withColumn(\"Content Length\", time_delta(F.col(\"strptime\")))\n",
    "    contentMapping = contentMapping.drop('strpTime')\n",
    "    contentMapping = contentMapping.withColumnRenamed(\"Title\", \"item_name\")\n",
    "    \n",
    "    #Merge df and contentMapping\n",
    "    df = df.join(contentMapping, [\"item_name\"], \"outer\")\n",
    "    \n",
    "    #get time played for\n",
    "    df = df.withColumn(\"Played For\", F.unix_timestamp(df[\"end\"]) - F.unix_timestamp(df[\"start\"]))\n",
    "    \n",
    "    #get total seconds of song as String, convert to bigInt\n",
    "    df = df.withColumn(\"Song Duration Str\", F.regexp_extract(df[\"Content Length\"], \"(?<=total: )(.*)(?= seconds)\", 0))\n",
    "    df = df.withColumn(\"Song Duration Int\", df[\"Song Duration Str\"].cast(IntegerType()))\n",
    "    \n",
    "\n",
    "    #Let's get Percentage Played\n",
    "    df = df.withColumn(\"PercentPlayed\", df[\"Played For\"] / df[\"Song Duration Int\"])\n",
    "    \n",
    "    #Let's keep only the columns we need at this point\n",
    "    df = df.select([\"device_id\", \"item_name\", \"PercentPlayed\"])\n",
    "\n",
    "    #assign weights based on Percent Played\n",
    "    df = df.withColumn(\n",
    "    'weight',\n",
    "    F.when((F.col(\"PercentPlayed\") >= 0.0) & (F.col(\"PercentPlayed\") < 0.25), uc.first)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.25) & (F.col(\"PercentPlayed\") < 0.50), uc.second)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.50) & (F.col(\"PercentPlayed\") < 0.75), uc.third)\\\n",
    "    .when((F.col(\"PercentPlayed\") >= 0.75) & (F.col(\"PercentPlayed\") <= 1.00), uc.fourth)\\\n",
    "    .otherwise(-999.999)\n",
    "    )\n",
    "    #drop the rows with invalid percent played\n",
    "    df = df.filter((df.weight != -999.999))\n",
    "\n",
    "    df.write.parquet(outputFile) # Write onto output Parquet\n",
    "\n",
    "\n",
    "# program start point\n",
    "if __name__ == \"__main__\":\n",
    "    ''' \n",
    "        Add arguments to script during execution on command line\n",
    "        Example of how to run the script: spark-submit template.py -i input.parquet -o output.parquet -c config.ini\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser() \n",
    "    parser.add_argument('-i', '--input', required=True)\n",
    "    parser.add_argument('-o', '--output', required=True)\n",
    "    parser.add_argument('-c', '--config', required=True)\n",
    "    parser.add_argument('-cm', '--contentmapping', required=True) #added on, so contentMapping is required\n",
    "    args = parser.parse_args()\n",
    "    config = ConfigParser()\n",
    "    config.read(args.config)\n",
    "    main(args.input,args.output,config, args.contentmapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
