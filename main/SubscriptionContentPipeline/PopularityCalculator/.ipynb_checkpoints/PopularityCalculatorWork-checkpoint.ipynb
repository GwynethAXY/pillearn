{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my imports, to make sure it works fine on Jupyter Notebook\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.context import SQLContext\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.functions import lit\n",
    "# from pyspark.sql.window import Window\n",
    "# import pyspark.sql.functions as F\n",
    "# from datetime import datetime\n",
    "# import pyspark.sql.types as T \n",
    "# from pyspark.sql.functions import split, explode\n",
    "\n",
    "#To get spark. working without throwing a NameError\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark as ps # Call this only after findspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#ACTUAL FUNCTION\n",
    "\n",
    "#for renaming the columns\n",
    "from functools import reduce\n",
    "\n",
    "#allow us to use SQL Count function for aggregation in groupBY\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "#allow us to read csv to dataframe\n",
    "import pandas as pd #need Pandas\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = testing = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Pop Goes the Weasel2\", \"2:00\"), \n",
    "    ],\n",
    "    ['Title', 'Length'] # add your columns label here\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+-------------------+-------------------+\n",
      "|device_id|           item_name|              start|                end|reach_end_of_stream|\n",
      "+---------+--------------------+-------------------+-------------------+-------------------+\n",
      "|        1|Pop Goes the Weas...|2005-06-01 13:33:00|2005-06-01 13:37:00|              false|\n",
      "|        1|Pop Goes the Weas...|2005-06-01 13:33:00|2005-06-01 13:34:00|               true|\n",
      "+---------+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "t = datetime.strptime(\"2:00\", \"%M:%S\")\n",
    "delta = timedelta(minutes=t.minute, seconds = t.second)\n",
    "\n",
    "testing = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Pop Goes the Weasel2.mp3\", \n",
    "         datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p'), \n",
    "         datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p'),\n",
    "        False), # create your data here, be consistent in the types.\n",
    "        (1, \"Pop Goes the Weasel2.mp3\", \n",
    "         datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p'), \n",
    "         datetime.strptime('Jun 1 2005  1:34PM', '%b %d %Y %I:%M%p'),\n",
    "        True),\n",
    "    ],\n",
    "    ['device_id', 'item_name', \"start\", \"end\", \"reach_end_of_stream\"] # add your columns label here\n",
    ")\n",
    "\n",
    "testing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.write.parquet('input.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blank_df = spark.range(0).drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_df.write.parquet(\"output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.write.parquet(\"contentMapping.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------+------+\n",
      "|device_id|item_name           |Percent Played|weight|\n",
      "+---------+--------------------+--------------+------+\n",
      "|1        |Pop Goes the Weasel2|2.0           |1.0   |\n",
      "|1        |Pop Goes the Weasel2|0.5           |0.66  |\n",
      "+---------+--------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "def main(inputFile, outputFile, configFile, contentMapping):\n",
    "    df = testing\n",
    "    contentMapping = mapping\n",
    "    \n",
    "    #get rid of \".mp3\" in item_name\n",
    "    df = df.withColumnRenamed(\"item_name\", \"to_del\")\n",
    "    df = df.withColumn(\"item_name\", F.split(df['to_del'], '\\.')[0])\n",
    "    df = df.drop('to_del')\n",
    "    \n",
    "    #turn Content Mapping String Length to TimeDelta Object\n",
    "    strp_time = udf (lambda x: datetime.strptime(x, \"%M:%S\"))\n",
    "    time_delta = udf (lambda y: timedelta(minutes = y.minute, seconds = y.second))\n",
    "    \n",
    "    contentMapping = contentMapping.withColumn(\"strptime\", strp_time(F.col(\"Length\")))\n",
    "    contentMapping = contentMapping.withColumn(\"Content Length\", time_delta(F.col(\"strptime\")))\n",
    "    contentMapping = contentMapping.drop('strpTime')\n",
    "    contentMapping = contentMapping.withColumnRenamed(\"Title\", \"item_name\")\n",
    "    \n",
    "    #Merge df and contentMapping\n",
    "    df = df.join(contentMapping, [\"item_name\"], \"outer\")\n",
    "    \n",
    "    #get time played for\n",
    "    df = df.withColumn(\"Played For\", F.unix_timestamp(df[\"end\"]) - F.unix_timestamp(df[\"start\"]))\n",
    "    \n",
    "    #get total seconds of song as String, convert to bigInt\n",
    "    df = df.withColumn(\"Song Duration Str\", F.regexp_extract(df[\"Content Length\"], \"(?<=total: )(.*)(?= seconds)\", 0))\n",
    "    df = df.withColumn(\"Song Duration Int\", df[\"Song Duration Str\"].cast(IntegerType()))\n",
    "    \n",
    "\n",
    "    #Let's get Percentage Played\n",
    "    df = df.withColumn(\"Percent Played\", df[\"Played For\"] / df[\"Song Duration Int\"])\n",
    "    \n",
    "    #Let's keep only the columns we need at this point\n",
    "    df = df.select([\"device_id\", \"item_name\", \"Percent Played\"])\n",
    "\n",
    "    #assign weights based on Percent Played\n",
    "    df = df.withColumn(\n",
    "    'weight',\n",
    "    F.when((F.col(\"Percent Played\") >= 0.0) & (F.col(\"Percent Played\") < 0.25), 0.0)\\\n",
    "    .when((F.col(\"Percent Played\") >= 0.25) & (F.col(\"Percent Played\") < 0.50), 0.33)\\\n",
    "    .when((F.col(\"Percent Played\") >= 0.50) & (F.col(\"Percent Played\") < 0.75), 0.66)\\\n",
    "    .otherwise(1.0)\n",
    "    )\n",
    "\n",
    "    \n",
    "    return df.show(2, False)\n",
    "    \n",
    "    #df = spark.read.parquet(inputFile+'/*').dropDuplicates().na.drop() Read from inputFile\n",
    "    #df.write.parquet(outputFile) Write onto output Parquet\n",
    "main(\"dd\", \"dd\", \"dd\", \"dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### common pyspark import statements\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T \n",
    "from pyspark.sql.functions import split, explode\n",
    "### other essnetial import statements\n",
    "import argparse\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#my imports\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "'''\n",
    "\tStandardize code to the following structure: everything to be in functions except pyspark envrionment setup\n",
    "\t\t- environment setup\n",
    "\t\t- user defined functions\n",
    "\t\t- main function\n",
    "\t\t- program start point\n",
    "''' \n",
    "\n",
    "# setup\n",
    "conf = SparkConf()\n",
    "conf.setAppName('pillar')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel('WARN')\n",
    "sql_context = SQLContext(sc)\n",
    "spark = SparkSession.builder.appName('pillar').getOrCreate()\n",
    "\n",
    "# user defined functions\n",
    "\n",
    "def userDefinedFunction1():\n",
    "    return\n",
    "\n",
    "def userDefinedFunction2():\n",
    "    return\n",
    "\n",
    "\n",
    "# main function: should always take in input file, output file and config file\n",
    "def main(inputFile, outputFile, configFile, contentMapping):\n",
    "    df = spark.read.parquet(inputFile+'/*').dropDuplicates().na.drop()\n",
    "    contentMapping = spark.read.parquet(contentMapping+'/*') #right formatting?\n",
    "    \n",
    "    #get rid of \".mp3\" in item_name\n",
    "    df = df.withColumnRenamed(\"item_name\", \"to_del\")\n",
    "    df = df.withColumn(\"item_name\", F.split(df['to_del'], '\\.')[0])\n",
    "    df = df.drop('to_del')\n",
    "    \n",
    "    #turn Content Mapping String Length to TimeDelta Object\n",
    "    strp_time = udf (lambda x: datetime.strptime(x, \"%M:%S\"))\n",
    "    time_delta = udf (lambda y: timedelta(minutes = y.minute, seconds = y.second))\n",
    "    \n",
    "    contentMapping = contentMapping.withColumn(\"strptime\", strp_time(F.col(\"Length\")))\n",
    "    contentMapping = contentMapping.withColumn(\"Content Length\", time_delta(F.col(\"strptime\")))\n",
    "    contentMapping = contentMapping.drop('strpTime')\n",
    "    contentMapping = contentMapping.withColumnRenamed(\"Title\", \"item_name\")\n",
    "    \n",
    "    #Merge df and contentMapping\n",
    "    df = df.join(contentMapping, [\"item_name\"], \"outer\")\n",
    "    \n",
    "    #get time played for\n",
    "    df = df.withColumn(\"Played For\", F.unix_timestamp(df[\"end\"]) - F.unix_timestamp(df[\"start\"]))\n",
    "    \n",
    "    #get total seconds of song as String, convert to bigInt\n",
    "    df = df.withColumn(\"Song Duration Str\", F.regexp_extract(df[\"Content Length\"], \"(?<=total: )(.*)(?= seconds)\", 0))\n",
    "    df = df.withColumn(\"Song Duration Int\", df[\"Song Duration Str\"].cast(IntegerType()))\n",
    "    \n",
    "\n",
    "    #Let's get Percentage Played\n",
    "    df = df.withColumn(\"Percent Played\", df[\"Played For\"] / df[\"Song Duration Int\"])\n",
    "    \n",
    "    #Let's keep only the columns we need at this point\n",
    "    df = df.select([\"device_id\", \"item_name\", \"Percent Played\"])\n",
    "\n",
    "    #assign weights based on Percent Played\n",
    "    df = df.withColumn(\n",
    "    'weight',\n",
    "    F.when((F.col(\"Percent Played\") >= 0.0) & (F.col(\"Percent Played\") < 0.25), 0.0)\\\n",
    "    .when((F.col(\"Percent Played\") >= 0.25) & (F.col(\"Percent Played\") < 0.50), 0.33)\\\n",
    "    .when((F.col(\"Percent Played\") >= 0.50) & (F.col(\"Percent Played\") < 0.75), 0.66)\\\n",
    "    .otherwise(1.0)\n",
    "    )\n",
    "\n",
    "    df.write.parquet(outputFile) # Write onto output Parquet\n",
    "\n",
    "\n",
    "# program start point\n",
    "if __name__ == \"__main__\":\n",
    "    ''' \n",
    "        Add arguments to script during execution on command line\n",
    "        Example of how to run the script: spark-submit template.py -i input.parquet -o output.parquet -c config.ini\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser() \n",
    "    parser.add_argument('-i', '--input', required=True)\n",
    "    parser.add_argument('-o', '--output', required=True)\n",
    "    parser.add_argument('-c', '--config', required=True)\n",
    "    parser.add_argument('-cm', '--contentmapping', required=True) #added on, so contentMapping is required\n",
    "    args = parser.parse_args()\n",
    "    config = ConfigParser()\n",
    "    config.read(args.config)\n",
    "    main(args.input,args.output,config, args.contentmapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
